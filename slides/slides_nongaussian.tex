\documentclass[11pt,handout,aspectratio=169]{beamer}


\input{./defs.tex}

\title[STA437-Week1]{STA 437/2005: \\ Methods for Multivariate Data}
\subtitle[]{Week 5: Non-Gaussian Distributions}
\author[Piotr Zwiernik]{Piotr Zwiernik}
\institute[UofT]{University of Toronto}
\date{}


%\usepackage{Sweave}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
\setbeamertemplate{section in toc}[sections numbered]
\tableofcontents%[hideallsubsections]
\end{frame}

\section{Elliptical distributions}

\begin{frame}{}
	\begin{center}
		{\Huge \alert{Elliptical distributions}}
	\end{center}
\end{frame}

\subsection{Spherical distributions}

\begin{frame}{Why Study Elliptical Distributions?}
    \begin{itemize}
        \item Generalize the multivariate normal distribution.\\[5mm]
        \item Model data with heavy tails or outliers.\\[5mm]
        \item Maintain symmetry and linear correlation structures.\\[5mm]
        \item Applications in finance, insurance, and environmental studies.
    \end{itemize}
\end{frame}

% Slide: Spherical Distributions Definition
\begin{frame}{Spherical Distributions}
\textbf{Orthogonal Matrices:} $O(m) = \{ U \in \mathbb{R}^{m \times m} : U^\top U = I_m \}$.
%  \begin{itemize}
%    \item $U \in O(m)$ implies $U^{-1} = U^\top$.
%    \item Determinants satisfy $|\det(U)| = 1$.
%  \end{itemize}
\begin{alertblock}{Spherical distribution}
A random vector $X \in \mathbb{R}^m$ has a \emph{spherical distribution} if for any $U \in O(m)$:
  \begin{equation*}
    X \overset{d}{=} U X.
  \end{equation*}	
\end{alertblock}
Characteristic function satisfies: $\psi_X(\bs t)=\psi_{UX}(\bs t)=\psi_X(U^\top \bs t)$ and so \textbf{equivalently} $\psi_X(\bs t)$ depends only on $\|\bs t\|$. Thus, the same applies to the density: $$f_X(\x)\;=\;h(\|\bs x\|)\qquad\mbox{for some }h\mbox{ (generator)}.$$
%e.g. $X\sim N_m(\bs 0_m,I_m)$ then $h(s)=\tfrac{1}{(2\pi)^{m/2}}e^{-\tfrac12 s}$
  \end{frame}

% Slide: Examples of Spherical Distributions
\begin{frame}{Examples of Spherical Distributions}
Standard normal distribution $Z \sim N_m(0, I_m)$ is a simple example.
\bigskip

\begin{block}{Spherical scale mixture of normals}
	If $Z \sim N_m(0, I_m)$ and a random variable $\tau > 0$ is independent of $Z$, then:
      \begin{equation*}
        X = \frac{1}{\sqrt{\tau}} Z
      \end{equation*}
      has a spherical distribution.
\end{block}
  \textbf{Indeed:} Let $U \in O(m)$, then 
$$    UX \;=\; \frac{1}{\sqrt{\tau}} UZ \;\overset{d}{=}\; \frac{1}{\sqrt{\tau}} Z \;=\; X.$$
\end{frame}

% Slide: Moment Structure of Spherical Distributions
\begin{frame}{Moment Structure of Spherical Distributions}
Spherical symmetry implies:
      \begin{align*}
        \mathbb{E}[X] &= 0, \\
        \var(X) &= c I_m, \quad \mbox{for some }c \geq 0.
      \end{align*}  
      \textbf{Indeed:} $\var(X)=\var(UX)=U \var(X)U^\top$ for any $U\in O(m)$ 
\bigskip

For $X = \frac{1}{\sqrt{\tau}} Z$ with $Z \sim N(0, I_m)$, $\tau>0$, $\tau\indep Z$:
      \begin{equation*}
        \var(X) = \mathbb{E}[\tau^{-1}] I_m.
      \end{equation*}
\textbf{Indeed:} $\E[X]=\E[\tfrac{1}{\sqrt{\tau}}Z]=\E[\tfrac{1}{\sqrt{\tau}}]\E[Z]=\bs 0_m$ and so 
$$\var(X)\;=\;\E XX^\top -\E[X]\E[X]^\top\;=\;\E[\tfrac{1}{\tau}ZZ^\top]\;=\;\E[\tfrac{1}{\tau}]\E[ZZ^\top]\;=\;\E[\tfrac{1}{\tau}]I_m$$
\end{frame}

% Slide: Independence of Norm and Direction
\begin{frame}{Independence of $\|X\|$ and $\frac{X}{\|X\|}$}
\begin{alertblock}{Key Property}
	If $X$ is spherical, the norm $\|X\| = \sqrt{X^\top X}$ is independent of the direction $\frac{X}{\|X\|}$.
\end{alertblock}
  \textbf{Proof Sketch:}
Let $U \in O(m)$. Then:
$$
        \frac{X}{\|X\|} \;\overset{d}{=}\; \frac{UX}{\|UX\|} \;=\; U \frac{X}{\|X\|}.
$$
The vector $\frac{X}{\|X\|}$ is rotationally invariant $\Longrightarrow$ has uniform distribution on the unit sphere (independent of what $\|X\|$ is).
\bigskip

A formal proof uses polar coordinates, see the notes.
\end{frame}

%% Slide: Polar Coordinates
%\begin{frame}{Polar Coordinates}
%  \textbf{Definition:} In $\mathbb{R}^m$, polar coordinates represent $\mathbf{x}$ as:
%  \begin{equation*}
%    \mathbf{x} = r \mathbf{u}(\boldsymbol{\theta}),
%  \end{equation*}
%  where $r = \|\mathbf{x}\|$ is the radial coordinate, and $\boldsymbol{\theta}$ are angular coordinates.
%  \vspace{0.5cm}
%  \textbf{Jacobian Determinant:}
%  \begin{equation*}
%    J(r, \boldsymbol{\theta}) = r^{m-1} \prod_{i=2}^{m-1} \sin^{m-i}(\theta_{i-1}).
%  \end{equation*}
%  \vspace{0.5cm}
%  \textbf{Implication:} If $f(\mathbf{x}) = g(\|\mathbf{x}\|^2)$, then:
%  \begin{equation*}
%    f(\mathbf{x}) d\mathbf{x} = g(r^2) r^{m-1} dr d\boldsymbol{\theta}.
%  \end{equation*}
%\end{frame}

\subsection{Elliptical distributions}

% Slide: Elliptical Distributions
\begin{frame}{Elliptical Distribution $E(\mu,\Sigma)$}
Recall that $Z\sim N_m(\bs 0_m,I_m)$ then $X=\mu+\Sigma^{1/2}Z\sim N_m(\mu,\Sigma)$.
\begin{block}{Elliptical distribution}
A random vector $X \in \mathbb{R}^m$ has an elliptical distribution if:
  \begin{equation*}
    X = \mu + \Sigma^{1/2} Z,
  \end{equation*}
  where $Z$ is a spherical random vector.	
\end{block}
The density of $X\sim E(\mu,\Sigma)$ is of the form $$f_X(\x)\;=\;c_m \sqrt{\det{\Sigma^{-1}}}h((\x-\mu)^\top\Sigma^{-1}(\x-\mu)).$$
The generator $g$ controls the shape of the distribution (and its tails in particular). 
\end{frame}

% Slide: Why Elliptical Distributions?
\begin{frame}{Again: Why Elliptical Distributions?}
    \begin{itemize}
        \item Generalize the multivariate normal distribution.\\[5mm]
        \item Model data with heavy tails or outliers.\\[5mm]
        \item Maintain symmetry and linear correlation structures.\\[5mm]
        \item Applications in finance, insurance, and environmental studies.
    \end{itemize}
\end{frame}

% Slide: Scale Mixtures of Normals
\begin{frame}{Scale Mixtures of Normals}
Scale mixture of normals is a special class of elliptical distributions. 
\bigskip

  \textbf{Stochastic representation:}
  \begin{equation*}
    X = \mu + \frac{1}{\sqrt{\tau}} \Sigma^{1/2} Z,
  \end{equation*}
  where $Z \sim N_m(0, I_m)$ and $\tau > 0$ is independent of $Z$.
  \vspace{0.5cm}
  \begin{block}{Special Cases of Scale Mixture of Normals}
  	  \begin{itemize}
    \item $\tau \equiv 1$: Multivariate normal.
    \item $\tau \sim \frac{1}{k} \chi^2_k$: Multivariate $t$-distribution with $k$ degrees of freedom.
    \begin{itemize}
    \item Smaller $k$ means heavier tails. Gaussian is the limit $k\to \infty$.
    \end{itemize}
    \item $\tau \sim \text{Exp}(1)$: Multivariate Laplace.
  \end{itemize}
    \end{block}
\end{frame}

% Slide: Covariance and Correlation
\begin{frame}{Covariance and Correlation in Elliptical Distributions}
$\Sigma$ is called the \textbf{scale matrix}. It is generally not equal to the covariance matrix.
\medskip
      \begin{equation*}
        \mathrm{Var}(X) = c \Sigma, \quad c > 0.
      \end{equation*}

Correlation structure is still governed by $\Sigma$:
$$
R_{ij}\;=\;\frac{c\Sigma_{ij}}{\sqrt{c\Sigma_{ii}c\Sigma_{jj}}}\;=\;\frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}}.
$$
Similarly, if $X\sim E(\mu,\Sigma)$ and $X=(X_A,X_B)$ then 
$$
\E(X_A|X_B=\x_B)\;=\;\E(X_A)-\Sigma_{A,B}\Sigma_{B,B}^{-1}(\x_B-\mu_B)
$$
exactly as in the Gaussian case.
\end{frame}

\section{Copula models}

\begin{frame}{}
	\begin{center}
		{\Huge \alert{Copula models}}
	\end{center}
\end{frame}

\begin{frame}{Cumulative Distribution Function (CDF)}
	Let $X=(X_1,\ldots,X_m)$ be a random vector.  Its \textbf{CDF} is: $$F(x_1,\ldots,x_m)=\P(X_1\leq x_1,X_2\leq x_2,\ldots,X_m\leq x_m).$$
	Marginal CDF: $F_1(x_1)=\P(X_1\leq x_1)=\lim_{x_2\to \infty}\cdots\lim_{x_m\to\infty}F(x_1,x_2,\ldots,x_m)$.
	(similar for any other margin)\\[5mm]	
	\begin{block}{}
	If $f$ is the corresponding density, then:
	$$
	f(x_1,\ldots,x_m)\;=\;\frac{\partial^m}{\partial x_1\cdots\partial x_m}F(x_1,\ldots,x_m)
	$$
	
	$$
	F(x_1,\ldots,x_m)\;=\;\int_{-\infty}^{x_1}\cdots \int_{-\infty}^{x_m} f(y_1,\ldots,y_m){\rm d}y_1\cdots {\rm d} y_m. 
	$$		
	\end{block}
		If $U\sim U[0,1]$ then $F(u)=u$ $u\in [0,1]$.

\end{frame}


\begin{frame}{What is a Copula?}
\begin{itemize}
    \item A \textbf{copula} is a function that captures the \textbf{dependence structure} between random variables, separate from their marginal distributions.
\end{itemize}
\begin{alertblock}{Definition}
	A function $C : [0, 1]^m \to [0, 1]$ is a \textbf{copula} if it is a CDF with uniform marginals, that is, $C_1(u_1)=u_1$, \ldots, $C_m(u_m)=u_m$, where $C_i$ are the marginal CDF's.
\end{alertblock}
For example, the copula $C(\bs u)=u_1\cdots u_m$ corresponds to a $m$ independent $U[0,1]$.
\begin{block}{Why use copulas?}
	\begin{itemize}
        \item To model non-Gaussian dependencies.
        \item To analyze dependence independently of marginal behaviors.
    \end{itemize}
\end{block}
\end{frame}

% Slide 2: Sklar's Theorem
\begin{frame}{Sklar's Theorem}
\begin{block}{Theorem (Sklar, 1959)}
Let $X = (X_1, \ldots, X_m)$ be a random vector with joint CDF $F$ and marginals $F_1, \ldots, F_m$. There exists a unique copula $C$ such that:
\[ F(x_1, \dots, x_m) = C(F_1(x_1), \dots, F_m(x_m)). \]
Conversely, given marginals $F_1, \ldots, F_m$ and a copula $C$, the joint CDF $F$ is defined by the same formula.
\end{block}
\begin{itemize}
    \item $C$ captures \textbf{dependence structure}.
    \item $F_1, \ldots, F_m$ capture marginal behaviors.
\end{itemize}
\end{frame}

% Slide 3: Understanding Sklar's Theorem
\begin{frame}{Understanding Sklar's Theorem}
\begin{itemize}
    \item When $m = 1$, $C(u) = u$, the identity function on $[0, 1]$.\\[5mm]
    \item If $X$ is continuous with CDF $F$, then $F(X) \sim U(0, 1)$. \\[3mm]
    \textbf{Proof:} If $X$ is continuous, $F$ is strictly increasing on the support. Hence
    \[
    \mathbb{P}(F(X) \leq u) = \mathbb{P}(X \leq F^{-1}(u)) = F(F^{-1}(u)) = u.
    \]
%    \item In higher dimensions:
%    \[ F(x_1, \ldots, x_m) = C(F_1(x_1), \ldots, F_m(x_m)). \]
\end{itemize}
\end{frame}

% Slide 4: Copulas from Uniform Marginals
\begin{frame}{}
\begin{itemize}
    \item Let $X = (X_1, \ldots, X_m)$ with CDF $F$.
    \item Define $U_i = F_i(X_i)$, where $F_i$ are the marginal CDFs.
    \item The transformed variables $U = (U_1, \ldots, U_m)$ have uniform marginals.
    \[ \mathbb{P}(U_1 \leq u_1, \ldots, U_m \leq u_m) = C(u_1, \ldots, u_m). \]
    \item Also note that 
\begin{equation}\label{eq:cop1}
    \mathbb{P}(U_1 \leq u_1, \ldots, U_m \leq u_m) = F(F_1^{-1}(u_1),\ldots,F_m^{-1}(u_m))	
\end{equation}
    and so the copula can be computed explicitly.
    \item Sklar's theorem ensures $C$ is unique for continuous distributions.
\end{itemize}
\begin{alertblock}{}
	Given copula $C$ and margins $F_i$ we easily get the corresponding  distribution using \eqref{eq:cop1}:
	$$
	F(x_1,\ldots,x_m)\;=\;C(F_1(x_1),\ldots,F_m(x_m)).
	$$
\end{alertblock}
\end{frame}

% Slide 5: Example of a Copula
\begin{frame}{Simple Example of a Copula}
\begin{itemize}
    \item Joint CDF:
    \[ F_{X,Y}(x, y) =
    \begin{cases} 
    0 & x < 0 \text{ or } y < 0, \\
    x^2 y^2 & 0 \leq x, y \leq 1, \\
    1 & x > 1 \text{ and } y > 1, \\
    \min(x^2, y^2) & \text{otherwise}.
    \end{cases} \]
    \item Marginal CDFs:
    \[ F_X(x) = x^2, \quad F_Y(y) = y^2 \quad \text{for } 0 \leq x, y \leq 1. \]
    \item Copula:
    \[ C(u, v) = uv \quad \text{if } u, v \leq 1. \]
\end{itemize}
\end{frame}


\begin{frame}{Sampling}
	Fix a copula $C(\bs u)$ and suppose we can sample from it.
	\begin{block}{Transform the copula sample}
		Consider a sample $\bs u^{(1)}$, \ldots, $\bs u^{(n)}$ from the copula.\\[3mm]
		
		Transform the data to have the right marginals $F_1,\ldots,F_m$:
		$$
		\x^{(t)}_i \;:=\; F_i^{-1}(\bs u^{(t)}_i)\qquad\mbox{for all }i=1,\ldots,m, t=1,\ldots,n.
		$$
		The sample $\x^{(1)},\ldots,\x^{(n)}$ has the right marginals and the right dependence structure. 
	\end{block}
	\medskip
	We will later show how to sample from some popular copula models. 
\end{frame}


% Slide 6: Invariance under Transformations
\begin{frame}{Invariance under Monotone Transformations}
Copulas are invariant under monotone transformations. 


\begin{alertblock}{}
	Consider $Y_i := f_i(X_i)$, where $f_i$ are strictly increasing transformations. Then the copula of $X$ is the same as the copula of $Y$. 
\end{alertblock}
Proof outline: Let $G$ be the CDF of $Y$ and $G_i$ the marginal CDF of $Y_i$
    \begin{itemize}
    \item By \eqref{eq:cop1}, equiv. show $F(F_1^{-1}(u_1), \ldots, F_m^{-1}(u_m))=G(G_1^{-1}(u_1), \ldots, G_m^{-1}(u_m))$
        \item $G_i(y_i)=\P(Y_i\leq y_i)=\P(f_i(X_i)\leq y_i)=\P(X_i\leq f_i^{-1}(y_i))=F_i(f_i^{-1}(y_i))$ and, in particular, $G_i^{-1}=f_i\circ F_i^{-1}$.
        \item Thus, $\{Y_i\leq G_i^{-1}(u_i)\}=\{f(X_i)\leq f_i(F^{-1}_i(u_i))\}=\{X_i\leq F^{-1}_i(u_i)\}$ and so  
      \begin{eqnarray*}
      	&&G(G_1^{-1}(u_1),\ldots, G_m^{-1}(u_m))= \P\left(\bigcap_{i=1}^m \{Y_i\leq G^{-1}_i(u_i)\}\right)\\
      	&=&\P\left(\bigcap_{i=1}^m \{X_i\leq F^{-1}_i(u_i)\}\right)=F(F_1^{-1}(u_1), \ldots, F_m^{-1}(u_m)).
      \end{eqnarray*} 
    \end{itemize}
\end{frame}

% Slide 7: Copula Density
\begin{frame}{Density of a Copula}
The PDF of a copula $C$ is obtained by differentiating its CDF:
    \[ c(\mathbf{u}) = \frac{\partial^m C(\mathbf{u})}{\partial u_1 \cdots \partial u_m}. \]
    
    Recall $C(\bs u)=F(F_1^{-1}(u_1),\ldots, F_m^{-1}(u_m))$. Using the chain rule:
    \[ c(\mathbf{u}) = \frac{f(\mathbf{x})}{\prod_{i=1}^m f_i(x_i)},\qquad \mbox{where } x_i=F_i^{-1}(u_i)\mbox{ for all }i \]
    where $f$ is the joint density and $f_i$ are marginal densities.
    \bigskip
    
    e.g. $C(\bs u)=u_1\cdots u_m$ is the CDF of independent $U_i\sim U(0,1)$. The density is uniform on $[0,1]^m$. Given margins $f_i$, we get $f(\x)=\prod_i f_i(x_i)$.
\end{frame}

% Slide 8: Gaussian Copula
\begin{frame}{Gaussian Copula}
Gaussian copula is derived from the multivariate normal distribution $X\sim N_m(\mu,\Sigma)$.\\[3mm]
By monotone invariance, we can assume $\E X_i=0$, $\var(X_i)=1$
\begin{itemize}
	\item  $\mu=0$, $\Sigma$ is a correlation matrix,
	\item each $X_i\sim N(0,1)$. 
\end{itemize}
 

Let $\Phi$ be the CDF of $N(0,1)$ with PDF $\phi$. Let $f(\x;\Sigma)$ be the PDF of $N_m(\bs 0,\Sigma)$.
\begin{alertblock}{The density of the Gaussian copula $C(\bs u;\Sigma)$}
	Using the general formula, we get:
	\[ c(\mathbf{u}; \Sigma) \;=\; \frac{f(\x;\Sigma)}{\prod_{i=1}^m \phi(x_i)}\;=\; \det(\Sigma)^{-1/2} \exp\left(-\frac{1}{2} \mathbf{x}^\top (\Sigma^{-1} - I_m) \mathbf{x}\right), \]
	where $x_i=\Phi^{-1}(u_i)$.
\end{alertblock}
 \end{frame}

\begin{frame}{Sampling from the Gaussian copula $C(\bs u;\Sigma)$}
	Sample $\z^{(1)},\ldots,\z^{(n)}\sim N_m(\bs 0,\Sigma)$.\\[3mm]
	
	Transform $\bs u^{(t)}_i=\Phi(\z^{(t)}_i)$ for all $i=1,\ldots,m$ and $t=1,\ldots,n$. \\[3mm]
	
	The sample $\bs u^{(1)},\ldots,\bs u^{(n)}$ comes from the Gaussian copula. \\[3mm]
	
	As described earlier, we can now transform this sample to get arbitrary margins.
\end{frame}

\begin{frame}{Steps to Estimate a Copula: normalize data}
	Given data $\x^{(1)},\ldots,\x^{(n)}$, start by fixing a copula model (e.g. Gaussian).\\[3mm]
	
	We assume the CDF $F$ of the data satisfies $F(\x)=C(F_1(x_1),\ldots,F_m(x_m))$.\\[3mm]
	
	However, \alert{the margins $F_i$ are not known!}.\\[3mm]
	
	Given a sample $\x_i^{(1)},\ldots,\x_i^{(n)}$ of $X_i$ we compute the \textbf{empirical CDF}
	$$
	\widehat F_i(x_i)\;:=\;\frac1n \sum_{t=1}^n \bs 1\{\x_i^{(t)}\leq x_i\}.
	$$
\begin{alertblock}{}
	Transform, the data using the empirical CDFs
$$
\bs u^{(t)}_i\;=\;\widehat F_i(\x^{(t)}_i).
$$
This transforms the data matrix $\X$ to $\bs U$ with uniform marginals. 
\end{alertblock}\end{frame}


\begin{frame}{Steps to Estimate a Copula: Fit the copula family}
In the next step, we fit the data to the given copula family. \\[3mm]

Often this is done by maximizing the log-likelihood $\sum_{t=1}^n \log c(\bs u^{(t)})$.\\[3mm]

In the case of the Gaussian copula $C(\bs u;\Sigma)$:
\begin{itemize}
	\item Transform the data to standard Gaussian margins: $\y^{(t)}_i=\Phi^{-1}(\bs u^{(t)}_i)$.
	\item Fit the Gaussian likelihood for $N_m(\bs 0,\Sigma)$ with the sample covariance  $S_n=\tfrac1n \Y^\top \Y$.
\end{itemize}
\end{frame}


\begin{frame}{Steps to Estimate a Copula: Evaluate the fit}
As the last step, compare the fitted copula model with the observed data. Check whether the copula captures the dependence structure accurately.\\[3mm]
We can generate samples from the fitted Gaussian copula.
\end{frame}




% Slide 9: Applications of Copulas
\begin{frame}{Applications of Copulas}
\begin{itemize}
    \item \textbf{Finance:} Modeling dependencies in asset returns.
    \item \textbf{Insurance:} Understanding risks in correlated claims.
    \item \textbf{Environmental Science:} Joint modeling of extreme events (e.g., floods).
    \item \textbf{Medical Statistics:} Modeling dependence in survival times.
\end{itemize}
\end{frame}


\section{Gaussian mixture models}

\begin{frame}{}
	\begin{center}
		{\Huge \alert{Gaussian mixtures}}
	\end{center}
\end{frame}

\section{Gaussian Mixture Models}

% Slide 1: Definition of GMMs
\begin{frame}{Mixture of Gaussians}
We combine simple models into a complex model by taking a mixture of $K$ multivariate Gaussian densities of the form:
$$
p(x)\;=\;\sum_{k=1}^K \pi_k N_m(x|\mu_k,\Sigma_k),
$$
where $\pi_k\geq 0$, $\sum_{k=1}^K\pi_k=1$, and $N_m(x|\mu_k,\Sigma_k)$ is the $m$-dim Gaussian density.
\begin{itemize}
	\item Each Gaussian component has its own mean vector $\mu_k$ and covariance matrix $\Sigma_k$.
	\item The parameters $\pi_k$ are called the mixing coefficients.
\end{itemize}
\pause
\begin{minipage}{7cm}{}
Example:
\begin{itemize}
	\item $K=3$ (three Gaussian components)
	\item $m=1$ (univariate Gaussians)
\end{itemize}
\end{minipage}\begin{minipage}{5cm}{}
	\begin{figure}
\includegraphics[scale=.25]{./pics/multimodal.jpg}
\end{figure}
\end{minipage}
\end{frame}

\begin{frame}{The crabs from Naples bay}

%	{In 1892, scientists collected data on populations of the crab and observed that the ratio of forehead width to the body length  actually showed a highly skewed distribution.}\\[.2cm]

%	\begin{figure*}
\begin{minipage}{5.5cm}{}
	\includegraphics[scale=.29]{pics/crabs.png}
		\end{minipage}\begin{minipage}{9cm}
			 In 1892, scientists collected data on populations of the crab and observed that the ratio of forehead width to the body length  actually showed a highly skewed distribution.\\[2mm] {\small Source: \textit{On Certain Correlated Variations in Carcinus maenas} (1893) W. F.  Weldon.}	
		\end{minipage}

{ They wondered whether this distribution could be the result of the population being a mix of two different normal distributions (two sub-species).}
\smallskip 

{ In \textbf{1894}, Karl Pearson proposed a method to fit this model (\href{https://archive.org/details/philtrans02543681}{\textcolor{blue}{read here}}), whose modern version is the ``method of moments''. The method involved solving a higher order polynomial.}
%	\end{figure*}
\end{frame}

\begin{frame}{}
%\frametitle{Mixture of Gaussians: 2D example}
\begin{figure}
\includegraphics[page=2,width=4.8in,trim={0 0 0 3cm},clip]{pics/raw.pdf}
\end{figure}
\end{frame}

% Slide 2: Why use Gaussian Mixtures?
\begin{frame}{Why Use Gaussian Mixtures?}
Gaussian Mixture Models (GMMs) are widely used because of their:
\begin{itemize}
    \item \textbf{Flexibility:} Ability to model complex data distributions.
    \item \textbf{Multimodality:} Handles datasets with multiple clusters or modes.
    \item \textbf{Interpretability:} Each Gaussian component represents a sub-population with interpretable parameters.
    \item \textbf{Clustering Applications:} GMMs are a natural probabilistic method for clustering.
\end{itemize}

\textbf{Special Case:}
For simplicity, in clustering, we often assume \( \Sigma_k = \Sigma \) for all \( k \).
\end{frame}

\begin{frame}
\frametitle{Mixture of Gaussians as a latent variable model}
Recall: \textcolor{blue}{$p(x)\;=\;\sum_{k=1}^K \pi_k N_m(x|\mu_k,\Sigma_k)$}.\\[.3cm]
\begin{itemize}
	\item Consider a latent variable $z$ with $K$ states $z\in \{1,\ldots,K\}$. 
%	\item For simplicity encode it in a 1-to-K representation:
%	$$
%	z\in \{e_1,\ldots,e_K\},\quad  \mbox{where } e_i=(0,\ldots,0,1,0,\ldots,0).
%	$$
	\item The distribution of $z$ given by the mixing coefficients: $$p(z=k)=\pi_k.$$
	\item Specify the conditional as $p(x|z=k)=N_m(x|\mu_k,\Sigma_k)$ with joint: $$p(x,z=k)\;=\;p(z=k)p(x|z=k)\;=\;\pi_k N_m(x|\mu_k,\Sigma_k).$$
	\item Then the marginal $p(x)$ satisfies $$\textcolor{blue}{p(x)=\sum_{k=1}^K p(x,z=k)\;=\;\sum_{k=1}^K \pi_k N_m(x|\mu_k,\Sigma_k)}.$$
\end{itemize}
%\begin{figure}
%\includegraphics[page=3,width=4.8in,trim={0 0 0 3cm},clip]{./raw.pdf}
%\end{figure}
\end{frame}

\begin{frame}{Yet another illustration}
The quantities $p(z|x)$ are called responsibilities.
\begin{figure}
\includegraphics[page=8,width=4.8in,trim={0 0 0 3cm},clip]{pics/raw.pdf}
\end{figure}
\end{frame}

% Slide 4: Likelihood Inference for GMMs
\begin{frame}{Likelihood Inference for GMMs}
\textbf{Objective:} Estimate the parameters \( \theta = \{ \pi_k, \mu_k, \Sigma_k \}_{k=1}^K \) by maximizing the likelihood:
\[
\ell(\theta) = \sum_{i=1}^n \log f(\mathbf{x}_i) = \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_k N_m(\mathbf{x}_i; \mu_k, \Sigma_k) \right).
\]

\textbf{Challenges:}
\begin{itemize}
    \item The likelihood function is multimodal and unbounded.
    \item Direct maximization is computationally difficult.
\end{itemize}

\textbf{Solution:} Use the \textbf{Expectation-Maximization (EM)} algorithm to iteratively estimate parameters using the latent variable representation.
\end{frame}

\begin{frame}{The General EM algorithm}
Consider a general setting with latent variables.
\begin{itemize}
	\item Observed dataset $\boldsymbol{X}\in \mathbb R^{N\times D}$, latent variables $\boldsymbol{Z}\in \mathbb R^{N\times K}$.
	\end{itemize}
Maximize the log-likelihood  \textcolor{purple}{$\log p(\boldsymbol{X}|\theta)=\log\left(\sum_{\boldsymbol Z}p(\boldsymbol X,\boldsymbol Z|\theta)\right)$}.
\begin{itemize}
	\item Initialize parameters $\theta^{\rm old}$.
%	\item Our knowledge on $z$ carried by the posterior $p(z|x,\theta)$. 
	 \item \textbf{E-step}: use $\theta^{\rm old}$ to compute the posterior $p(\boldsymbol{Z}|\boldsymbol{X},\theta^{\rm old})$.
 	\item \textbf{M-step}: $\theta^{\rm new}=\arg\max_\theta Q(\theta,\theta^{\rm old})$, where
$$
	Q(\theta,\theta^{\rm old})\;=\;\sum_{\boldsymbol{Z}}p(\boldsymbol{Z}|\boldsymbol{X},\theta^{\rm old})\log p(\boldsymbol{X},\boldsymbol{Z}|\theta)\;=\;\mathbb E\Big(\log p(\boldsymbol{X},\boldsymbol{Z}|\theta)\Big|\boldsymbol{X},\theta^{\rm old}\Big)
$$
which is tractable in many applications.
\item Replace $\theta^{\rm old}\leftarrow\theta^{\rm new}$. Repeat until convergence.
\end{itemize}
%\begin{figure}
%\includegraphics[page=19,width=4.8in,trim={0 0 0 3cm},clip]{./raw.pdf}
%\end{figure}
\end{frame}

\begin{frame}{Example: Gaussian mixture}
\begin{itemize}
%	\item We confirm earlier formulas for the Gaussian mixture.
	\item If $z$ was observed, the MLE would be trivial
	$${\small \log p(\boldsymbol X,\boldsymbol Z|\theta)=\sum_{n=1}^N \log p(x_n,z_n|\theta)=\sum_{n=1}^N\sum_{k=1}^K \textcolor{blue}{1\!\!1(z_n\!=\!k)}\log\left(\pi_kN(x_n|\mu_k,\Sigma_k)\right).}$$
\end{itemize}
For the E-step: $p(\boldsymbol{Z}|\boldsymbol{X},\theta)=\prod_{n=1}^N p(z_n|\boldsymbol{X},\theta)$ we have
$$
p(z_n=k|\boldsymbol{X},\theta)=p(z_n=k|x_n,\theta)=\frac{\pi_k N_m(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^K \pi_j N_m(x_n|\mu_j,\Sigma_j)}.
$$
For the M-step: ${\small \mathbb E(1\!\!1(z_n=k)|\boldsymbol{X},\theta^{\rm old})=p(z_n=k|\boldsymbol{X},\theta^{\rm old})}$ and so 
$$
{\small \mathbb E\Big(\log p(\boldsymbol X,\boldsymbol Z|\theta)\Big|\boldsymbol{X},\theta^{\rm old}\Big)\;=\;\sum_{n=1}^N\sum_{k=1}^K \textcolor{blue}{p(z_n=k|\boldsymbol{X},\theta^{\rm old})}\log\left(\pi_kN(x_n|\mu_k,\Sigma_k)\right).}
$$
Maximizing gives the formulas on Slide~\ref{GMEM}.
%\begin{figure}
%\includegraphics[page=24,width=4.8in,trim={0 0 0 3cm},clip]{./raw.pdf}
%\end{figure}
\end{frame}



% Slide 5: EM Algorithm for GMMs
\begin{frame}{EM Algorithm for GMMs}
The EM algorithm alternates between two steps to estimate parameters:
\begin{enumerate}
    \item \textbf{E-step:} Compute the posterior probabilities (responsibilities) for each component:
    \[
    w_{ik} = \mathbb{P}(Z = k \mid \mathbf{X} = \mathbf{x}_i, \theta) = 
    \frac{\pi_k N_m(\mathbf{x}_i; \mu_k, \Sigma_k)}{\sum_{l=1}^K \pi_l N_m(\mathbf{x}_i; \mu_l, \Sigma_l)}.
    \]

    \item \textbf{M-step:} Update the parameters using the responsibilities:
    \begin{align*}
        \pi_k &= \frac{1}{n} \sum_{i=1}^n w_{ik}, \\
        \mu_k &= \frac{\sum_{i=1}^n w_{ik} \mathbf{x}_i}{\sum_{i=1}^n w_{ik}}, \\
        \Sigma_k &= \frac{\sum_{i=1}^n w_{ik} (\mathbf{x}_i - \mu_k)(\mathbf{x}_i - \mu_k)^\top}{\sum_{i=1}^n w_{ik}}.
    \end{align*}
\end{enumerate}

\textbf{Convergence:} Repeat until the log-likelihood \( \ell(\theta) \) converges.
\end{frame}




% Slide 6: Example: GMM in Action
\begin{frame}{Example: GMM in Action}
\textbf{Simulated Data:} Consider a 2D dataset generated from two Gaussian components:
\[
\mu_1 = \begin{bmatrix} 2 \\ 2 \end{bmatrix}, \quad
\Sigma_1 = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}, \quad
\pi_1 = 0.4,
\]
\[
\mu_2 = \begin{bmatrix} 7 \\ 7 \end{bmatrix}, \quad
\Sigma_2 = \begin{bmatrix} 1 & -0.3 \\ -0.3 & 1 \end{bmatrix}, \quad
\pi_2 = 0.6.
\]

\begin{itemize}
    \item Use the EM algorithm to estimate \( \pi_k, \mu_k, \Sigma_k \).
    \item Visualize posterior probabilities after 1, 2, and 30 iterations.
\end{itemize}

\begin{alertblock}{Questions to Consider:}
    \begin{itemize}
        \item How close are the estimated parameters to the true values?
        \item How does the choice of \( K \) affect results?
    \end{itemize}
\end{alertblock}
\end{frame}

% Slide 7: Visualization of EM Algorithm
\begin{frame}{Visualization of EM Algorithm}
\begin{itemize}
    \item Initial parameter estimates result in poor cluster separation.
    \item Responsibilities evolve over iterations, improving separation.
    \item Log-likelihood increases monotonically with iterations.
\end{itemize}

\includegraphics[width=0.3\textwidth]{pics/GMM2_1.pdf}\includegraphics[width=0.3\textwidth]{pics/GMM2_2.pdf}\includegraphics[width=0.3\textwidth]{pics/GMM2_3.pdf}
\end{frame}

\end{document}

